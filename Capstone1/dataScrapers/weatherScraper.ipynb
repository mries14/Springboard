{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 'wildcard-weekend', 'divisional-playoffs', 'conf-championships', 'superbowl']\n"
     ]
    }
   ],
   "source": [
    "# Firstly we need to collect all links that we intend to scrape from\n",
    "# the data needed is in the format of:\n",
    "# https://www.pro-football-reference.com/years/2018/week_21.htm\n",
    "# We need to set up a dictionary containing all weeks and years: seasonDict\n",
    "\n",
    "# First setup years and weeks we are interested in. Note that the\n",
    "# postseason starts in week 18 and the superbowl is week 21\n",
    "# We will collect dating back to 2004\n",
    "yearNum = [year for year in range(2009,2019)]\n",
    "weekNum = [week for week in range(1,18)]\n",
    "extraWeeks = ['wildcard-weekend','divisional-playoffs','conf-championships','superbowl']\n",
    "for week in extraWeeks:\n",
    "    weekNum.append(week)\n",
    "seasonDict = {i : [j for j in weekNum] for i in yearNum}\n",
    "print(seasonDict[2016])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://www.nflweather.com/en/week/2016/week-1/', 'http://www.nflweather.com/en/week/2016/week-2/', 'http://www.nflweather.com/en/week/2016/week-3/', 'http://www.nflweather.com/en/week/2016/week-4/', 'http://www.nflweather.com/en/week/2016/week-5/', 'http://www.nflweather.com/en/week/2016/week-6/', 'http://www.nflweather.com/en/week/2016/week-7/', 'http://www.nflweather.com/en/week/2016/week-8/', 'http://www.nflweather.com/en/week/2016/week-9/', 'http://www.nflweather.com/en/week/2016/week-10/']\n"
     ]
    }
   ],
   "source": [
    "# Now we can build the links we need, start with empty list: seasonLinks\n",
    "seasonLinks = []\n",
    "\n",
    "# Now iterate through seasonDict and put together all needed links\n",
    "# remeber the format of the links is: https://www.pro-football-reference.com/years/2018/week_21.htm\n",
    "for year in seasonDict.keys():\n",
    "    # the website has a different format for the year 2010 than other weeks, so we need to adjust accordingly\n",
    "    # format: http://www.nflweather.com/en/week/2010/week-6-2/\n",
    "    if year == 2010:\n",
    "        for week in seasonDict[year]:\n",
    "        # the non-integer weeks, such as superbowl week, are formatted differently\n",
    "            if isinstance(week, int) == True:\n",
    "                seasonLinks.append('http://www.nflweather.com/en/week/' + str(year) + '/week-' + str(week) + '-2/')\n",
    "            else:\n",
    "                seasonLinks.append('http://www.nflweather.com/en/week/' + str(year) + '/' + week + '-2/')\n",
    "    else:\n",
    "        for week in seasonDict[year]:\n",
    "            # the non-integer weeks, such as superbowl week, are formatted differently\n",
    "            if isinstance(week, int) == True:\n",
    "                seasonLinks.append('http://www.nflweather.com/en/week/' + str(year) + '/week-' + str(week) + '/')\n",
    "            else:\n",
    "                seasonLinks.append('http://www.nflweather.com/en/week/' + str(year) + '/' + week + '/')\n",
    "\n",
    "print(seasonLinks[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to setup a csv file with column names to hold our data\n",
    "csvFile = open(\"./csvFiles/weatherData2.csv\", 'w+')\n",
    "writer = csv.writer(csvFile)\n",
    "writer.writerow(('season','week','awayTeam','homeTeam','forecast','wind'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chiefs\n",
      "Raiders\n",
      "61f Partly Cloudy\n",
      "4m SW\n"
     ]
    }
   ],
   "source": [
    "# First, let's scrape data from one page\n",
    "\n",
    "url = 'http://www.nflweather.com/en/week/2017/week-7/'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "\n",
    "# Grab the stadium and date. Strip of leading and trailing whitespaces\n",
    "awayTeam = soup.select(\"tbody tr td\")[1].text.strip()\n",
    "homeTeam = soup.select(\"tbody tr td\")[5].text.strip()\n",
    "forecast = soup.select(\"tbody tr td\")[9].text.strip()\n",
    "wind = soup.select(\"tbody tr td\")[11].text.strip()\n",
    "\n",
    "print(awayTeam)\n",
    "print(homeTeam)\n",
    "print(forecast)\n",
    "print(wind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.nflweather.com/en/week/2009/wildcard-weekend/\n",
      "http://www.nflweather.com/en/week/2009/divisional-playoffs/\n",
      "http://www.nflweather.com/en/week/2009/conf-championships/\n",
      "http://www.nflweather.com/en/week/2009/superbowl/\n",
      "http://www.nflweather.com/en/week/2010/week-1-2/\n",
      "http://www.nflweather.com/en/week/2010/week-2-2/\n",
      "http://www.nflweather.com/en/week/2010/week-3-2/\n",
      "http://www.nflweather.com/en/week/2010/week-4-2/\n",
      "http://www.nflweather.com/en/week/2010/week-5-2/\n",
      "http://www.nflweather.com/en/week/2010/week-6-2/\n",
      "http://www.nflweather.com/en/week/2010/week-7-2/\n",
      "http://www.nflweather.com/en/week/2010/week-8-2/\n",
      "http://www.nflweather.com/en/week/2010/week-9-2/\n",
      "http://www.nflweather.com/en/week/2010/week-10-2/\n",
      "http://www.nflweather.com/en/week/2010/week-11-2/\n",
      "http://www.nflweather.com/en/week/2010/week-12-2/\n",
      "http://www.nflweather.com/en/week/2010/week-13-2/\n",
      "http://www.nflweather.com/en/week/2010/week-14-2/\n",
      "http://www.nflweather.com/en/week/2010/week-15-2/\n",
      "http://www.nflweather.com/en/week/2010/week-16-2/\n"
     ]
    }
   ],
   "source": [
    "# Now loop through and write all collected data from this page into our csv file as one row:\n",
    "\n",
    "for game in seasonLinks:\n",
    "    url = game\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "    \n",
    "    # need to grab from every row in the table for each week\n",
    "    allRows = soup.select(\"tbody tr\")\n",
    "    \n",
    "    for i in range(len(allRows)):\n",
    "        awayTeam = allRows[i].select('td')[1].text.strip()\n",
    "        homeTeam = allRows[i].select('td')[5].text.strip()\n",
    "        forecast = allRows[i].select('td')[9].text.strip()\n",
    "        wind = allRows[i].select('td')[11].text.strip()\n",
    "\n",
    "        writer.writerow((game,game,awayTeam,homeTeam,forecast,wind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleaned script: \n",
    "\n",
    "import csv\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "import requests\n",
    "\n",
    "yearNum = [year for year in range(2009,2019)]\n",
    "weekNum = [week for week in range(1,18)]\n",
    "extraWeeks = ['wildcard-weekend','divisional-playoffs','conf-championships','superbowl']\n",
    "for week in extraWeeks:\n",
    "    weekNum.append(week)\n",
    "seasonDict = {i : [j for j in weekNum] for i in yearNum}\n",
    "\n",
    "seasonLinks = []\n",
    "\n",
    "for year in seasonDict.keys():\n",
    "    if year == 2010:\n",
    "        for week in seasonDict[year]:\n",
    "            if isinstance(week, int) == True:\n",
    "                seasonLinks.append('http://www.nflweather.com/en/week/' + str(year) + '/week-' + str(week) + '-2/')\n",
    "            else:\n",
    "                seasonLinks.append('http://www.nflweather.com/en/week/' + str(year) + '/' + week + '-2/')\n",
    "    else:\n",
    "        for week in seasonDict[year]:\n",
    "            # the non-integer weeks, such as superbowl week, are formatted differently\n",
    "            if isinstance(week, int) == True:\n",
    "                seasonLinks.append('http://www.nflweather.com/en/week/' + str(year) + '/week-' + str(week) + '/')\n",
    "            else:\n",
    "                seasonLinks.append('http://www.nflweather.com/en/week/' + str(year) + '/' + week + '/')\n",
    "\n",
    "                \n",
    "csvFile = open(\"./csvFiles/weatherData4.csv\", 'w+')\n",
    "writer = csv.writer(csvFile)\n",
    "writer.writerow(('season','week','awayTeam','homeTeam','forecast','wind'))\n",
    "\n",
    "for week in seasonLinks:\n",
    "    url = week\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "    allRows = soup.select(\"tbody tr\")\n",
    "    \n",
    "    for i in range(len(allRows)):\n",
    "        awayTeam = allRows[i].select('td')[1].text.strip()\n",
    "        homeTeam = allRows[i].select('td')[5].text.strip()\n",
    "        forecast = allRows[i].select('td')[9].text.strip()\n",
    "        wind = allRows[i].select('td')[11].text.strip()\n",
    "\n",
    "        writer.writerow((week,week,awayTeam,homeTeam,forecast,wind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
